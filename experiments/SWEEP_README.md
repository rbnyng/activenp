# Active Learning Sweep Experiments

This directory contains tools for running comprehensive sweep experiments to evaluate active learning strategies across different initial seed sizes and random seeds.

## Overview

The sweep experiments answer key questions:
- How does active learning performance vary with initial seed size?
- At what point does uncertainty sampling provide maximum advantage?
- How robust are the results across different random data splits?
- What is the sample efficiency of different strategies?

## Quick Start

### 1. Run a Small Test Sweep

Test the setup with a minimal configuration:

```bash
python experiments/sweep_experiment.py \
    --seed-sizes 10 25 \
    --n-random-seeds 3 \
    --strategies random uncertainty \
    --output-dir ./results/test_sweep \
    --dry-run
```

Remove `--dry-run` to actually run the experiments.

### 2. Run Full Sweep

Run the complete experimental sweep:

```bash
python experiments/sweep_experiment.py \
    --seed-sizes 5 10 25 100 1000 \
    --n-random-seeds 10 \
    --strategies random uncertainty spatial hybrid \
    --output-dir ./results/sweep \
    --device cuda
```

This will run **200 experiments** (5 seed sizes × 10 random seeds × 4 strategies).

### 3. Run in Parallel

Speed up execution with parallel workers:

```bash
python experiments/sweep_experiment.py \
    --seed-sizes 5 10 25 100 1000 \
    --n-random-seeds 10 \
    --strategies random uncertainty spatial hybrid \
    --output-dir ./results/sweep \
    --parallel \
    --n-workers 4 \
    --device cuda
```

**Note**: Parallel mode uses multiprocessing. For multi-GPU setups, you'll need to modify the script to assign different GPUs to different workers.

### 4. Analyze Results

Once experiments complete, analyze and visualize:

```bash
python experiments/analyze_sweep.py ./results/sweep
```

This generates:
- Learning curves with error bands for each seed size
- Sample efficiency comparison (samples needed to reach target RMSE)
- Cold start advantage plots (uncertainty vs random)
- Statistical significance tests (paired t-tests)
- Summary tables

## Output Structure

```
results/sweep/
├── sweep_config.json           # Configuration used for sweep
├── sweep_summary.json          # Summary of experiment runs (success/failure)
├── seed_5/
│   ├── run_0/                  # Random seed 0
│   │   ├── config.json
│   │   ├── results.json
│   │   ├── random/
│   │   │   └── checkpoint_iter_*.pt
│   │   ├── uncertainty/
│   │   ├── spatial/
│   │   └── hybrid/
│   ├── run_1/                  # Random seed 1
│   └── ...
├── seed_10/
├── seed_25/
├── seed_100/
├── seed_1000/
└── analysis/                   # Generated by analyze_sweep.py
    ├── summary_table.csv
    ├── sample_efficiency.csv
    ├── statistical_tests.csv
    ├── learning_curves_test_rmse.png
    ├── learning_curves_test_r2.png
    ├── sample_efficiency.png
    ├── cold_start_advantage_test_rmse.png
    └── cold_start_advantage_test_r2.png
```

## Sweep Configuration

### Seed Sizes

Default: `5, 10, 25, 100, 1000`

These represent realistic conservation scenarios:
- **5-25 samples**: Extreme few-shot regime (single field campaign)
- **100 samples**: Moderate initial data
- **1000 samples**: Well-resourced project

Customize with: `--seed-sizes 5 10 25 100`

### Random Seeds

Default: `10` runs per configuration

This provides statistical robustness:
- Mean and SEM across runs
- Paired statistical tests
- Error bands on learning curves

Customize with: `--n-random-seeds 10`

### Sampling Strategies

Default: `random, uncertainty, spatial, hybrid`

- **Random**: Baseline random sampling
- **Uncertainty**: Select highest predictive uncertainty
- **Spatial**: Maximize geographic diversity (maxmin)
- **Hybrid**: Top uncertain candidates + spatial diversity

Customize with: `--strategies random uncertainty hybrid`

### Total Sample Budget

Default: `250` samples

For small seed sizes, this allows enough iterations to see learning curves.
For large seed sizes (e.g., 1000), the script automatically adjusts to add 500 more samples.

Customize with: `--total-samples 250`

### Region

Default: Maine, USA (`-70.0 44.0 -69.0 45.0`)

Customize with: `--bbox lon_min lat_min lon_max lat_max`

For multi-region experiments, run separate sweeps for different bounding boxes.

## Analysis Outputs

### 1. Learning Curves

**File**: `learning_curves_test_rmse.png`, `learning_curves_test_r2.png`, etc.

Shows test RMSE/R²/MAE vs number of training samples for each seed size and strategy.
- Lines show mean across random seeds
- Shaded regions show mean ± SEM

**Interpretation**: Lower curves (RMSE) or higher curves (R²) are better. Steeper initial slopes indicate faster learning.

### 2. Sample Efficiency

**File**: `sample_efficiency.png`, `sample_efficiency.csv`

Bar chart showing samples needed to reach target RMSE (default: 0.5) for each strategy.

**Interpretation**: Lower bars mean more sample-efficient. Quantifies "How many fewer samples does uncertainty sampling need vs random?"

### 3. Cold Start Advantage

**File**: `cold_start_advantage_test_rmse.png`

Plots (random_RMSE - uncertainty_RMSE) vs initial seed size.

**Interpretation**:
- Positive values: uncertainty is better
- Should be highest at low seed sizes (cold start problem)
- Should decrease as seed size increases (diminishing returns)

### 4. Statistical Tests

**File**: `statistical_tests.csv`

Paired t-tests comparing strategies at each seed size:
- p-value < 0.05: statistically significant difference
- Cohen's d: effect size (small: 0.2, medium: 0.5, large: 0.8)

### 5. Summary Table

**File**: `summary_table.csv`

Final metrics (mean ± SEM) for each configuration:
- Final number of training samples
- Test RMSE, MAE, R²
- Number of runs

## Computational Requirements

### Time Estimates

Single experiment (100 seed, 15 iterations, Maine):
- ~30-60 minutes on GPU
- ~2-4 hours on CPU

Full sweep (200 experiments):
- Sequential: ~100-200 GPU-hours
- Parallel (4 workers): ~25-50 GPU-hours

### Resource Requirements

- **GPU memory**: ~4-8 GB per worker
- **Disk space**: ~500 MB per experiment (with checkpoints)
- **Total for full sweep**: ~100 GB

### Optimization Tips

1. **Use parallel execution**: `--parallel --n-workers 4`
2. **Reduce checkpointing**: Modify `simple_loop.py` to save fewer checkpoints
3. **Use smaller regions**: Reduce bounding box size for faster GEDI queries
4. **Reduce iterations**: For initial testing, use `--total-samples 150`

## Cluster/SLURM Usage

For HPC clusters, you can submit individual experiments as array jobs:

```bash
#!/bin/bash
#SBATCH --array=0-199
#SBATCH --gres=gpu:1
#SBATCH --time=02:00:00

# Parse SLURM_ARRAY_TASK_ID to get seed_size, random_seed, strategy
# Then run single experiment with appropriate flags

python experiments/active_learning_experiment.py \
    --n-seed ${SEED_SIZE} \
    --strategies ${STRATEGY} \
    --output-dir results/sweep/seed_${SEED_SIZE}/run_${RANDOM_SEED} \
    --device cuda
```

See `sweep_experiment.py` for generating configurations programmatically.

## Multi-Region Experiments

To test generalization across biomes:

```bash
# Temperate forest (Maine)
python experiments/sweep_experiment.py \
    --bbox -70.0 44.0 -69.0 45.0 \
    --output-dir ./results/sweep_maine \
    --device cuda

# Tropical forest (Amazon)
python experiments/sweep_experiment.py \
    --bbox -73.0 2.9 -72.9 3.0 \
    --output-dir ./results/sweep_amazon \
    --device cuda

# African savanna
python experiments/sweep_experiment.py \
    --bbox 36.0 -3.0 37.0 -2.0 \
    --output-dir ./results/sweep_africa \
    --device cuda
```

Then analyze each separately or combine results for cross-biome analysis.

## Troubleshooting

### Experiments failing

Check `sweep_summary.json` for failed runs and error messages.

Common issues:
- Insufficient GEDI data in region: Use larger bounding box
- Out of memory: Reduce batch size in config or use CPU
- Missing embeddings: Check GeoTessera cache

### Incomplete results

If sweep is interrupted:
- Re-run with same config - completed experiments are skipped (they're in separate directories)
- Or manually remove incomplete `seed_X/run_Y` directories and re-run

### Analysis errors

If `analyze_sweep.py` fails:
- Check that all experiments have `results.json` files
- Ensure at least one strategy completed successfully
- Check for consistent metric names across runs

## Publication Workflow

1. **Run full sweep** with 10 random seeds
2. **Generate all plots** with `analyze_sweep.py`
3. **Create main figure**: Learning curves for key seed sizes (5, 25, 100)
4. **Create supplementary**: All metrics, all seed sizes
5. **Report statistics**: Include p-values and Cohen's d in text
6. **Sample efficiency table**: Show samples saved by uncertainty vs random

## Advanced Customization

### Modifying the Experiment

Edit `active_learning_experiment.py` to change:
- Model hyperparameters
- Training epochs
- Evaluation metrics

### Custom Strategies

Add new sampling strategies in `active_learning/strategies.py`:

```python
class CustomSampler(SamplingStrategy):
    def __init__(self):
        super().__init__("custom")

    def select_samples(self, pool_indices, pool_coords, n_samples,
                      uncertainties=None, train_coords=None):
        # Your selection logic here
        return selected_indices
```

Then add to `--strategies custom` in sweep command.

### Custom Analysis

The analysis script provides aggregated data that you can use for custom plots:

```python
from analyze_sweep import load_sweep_results, aggregate_runs

results = load_sweep_results(Path('./results/sweep'))

# Access aggregated data
for seed_size, strategies in results.items():
    for strategy, data in strategies.items():
        runs = data['runs']
        agg = aggregate_runs(runs)

        # agg contains: n_train_mean, test_rmse_mean, test_rmse_sem, etc.
        # Create custom visualizations
```

## Questions?

For issues or questions:
- Check experiment logs in `seed_X/run_Y/*/`
- Review `sweep_summary.json` for experiment status
- Examine individual `results.json` files for detailed metrics
